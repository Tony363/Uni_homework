{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ENGI E1006: Introduction to Computing for Engineers and Applied Scientists\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` is a powerful library built on `numpy` that implements a large number of **Machine Learning** algorithms and tools.\n",
    "\n",
    "\n",
    "Part of `scikit-learn`'s power is its simple, polymorphic interface. Most models have the same interface:\n",
    "\n",
    "```python\n",
    "# instantiate the model\n",
    "model = ScikitLearnModel(model_specific_parameters...)\n",
    "\n",
    "# fit the model to the training data\n",
    "model.fit(KnownData, KnownDataLabels)\n",
    "\n",
    "# predict new data\n",
    "model.predict(UnknownData)\n",
    "```\n",
    "\n",
    "This flexibility lets us build a lot of powerful tools, with very little code!\n",
    "\n",
    "In this notebook, we will explore some of scikit-learn's functionality using the **Iris** dataset. This is a classic dataset from machine learning. More information can be found here: https://en.wikipedia.org/wiki/Iris_flower_data_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn # notice that scikit-learn's import name is sklearn!\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn is built on `numpy`, but we can leverage `pandas` and `seaborn` to do some **exploratory data analysis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df_orig = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "df_orig['target'] = iris_data.target\n",
    "df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's swap target for target name\n",
    "df = df_orig.copy()\n",
    "df['target'] = df['target'].map({i: iris_data.target_names[i] for i in range(len(iris_data.target_names))})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do some plotting\n",
    "sns.pairplot(df, hue='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool!\n",
    "\n",
    "This might seem like just \"for fun\", but our data analysis has shown us something super important. Looking at the pairplot, it shows that our dataset appears seperable, meaning it looks like plenty of algorithms will work on it. If you recall the randomly generated dataset from previous lectures, it exhibited almost no seperability, meaning we'd have a very difficult time constructing an algorithm around it.\n",
    "\n",
    "Let's go ahead and leverage scikit-learn's **K Nearest Neihbors** algorithm. In order to judge its accuracy, we'll separate some of our data for later testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_data = df_orig.to_numpy()\n",
    "all_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the targets are in order, lets shuffle our data\n",
    "np.random.shuffle(all_data)\n",
    "all_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's reserve 20% for testing\n",
    "training_data = all_data[:int(.8*len(all_data))]\n",
    "testing_data = all_data[int(.8*len(all_data)):]\n",
    "\n",
    "print(len(all_data), len(training_data), len(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally lets separate the labels\n",
    "training_data_labels = training_data[:,-1:] # grab just the labels column\n",
    "training_data_labels = training_data_labels.reshape(len(training_data_labels)) # reshape as vector\n",
    "training_data = training_data[:,:-1] # slice out the labels column\n",
    "\n",
    "print(training_data_labels.shape, training_data.shape)\n",
    "print(training_data_labels[:1], training_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and for testing data\n",
    "testing_data_labels = testing_data[:,-1:] # grab just the labels column\n",
    "testing_data_labels = testing_data_labels.reshape(len(testing_data_labels)) # reshape as vector\n",
    "testing_data = testing_data[:,:-1] # slice out the labels column\n",
    "print(testing_data_labels.shape, testing_data.shape)\n",
    "print(testing_data_labels[:1], testing_data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Now we're ready to construct our KNN model. For this, lets use `K = 5`.\n",
    "\n",
    "Scikit-learn has extensive documentation on every model. The KNN docs are here: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# first, we instantiate our model\n",
    "model = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we train the model on the training data and training labels\n",
    "model.fit(training_data, training_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we generate a numpy array of predictions for the testing data\n",
    "predicted_test_labels = model.predict(testing_data)\n",
    "predicted_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, we can do a simple comparison to the known test labels to get an accuracy\n",
    "sum(predicted_test_labels == testing_data_labels) / len(testing_data_labels) * 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! 100% accuracy. Though looking at the scatter matrix, this makes sense. The data is very seperated. What if we tried just the 1 nearest neighbor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(training_data, training_data_labels)\n",
    "sum(model.predict(testing_data) == testing_data_labels) / len(testing_data_labels) * 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just for fun, what if we picked too many neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=100)\n",
    "model.fit(training_data, training_data_labels)\n",
    "sum(model.predict(testing_data) == testing_data_labels) / len(testing_data_labels) * 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense, because we don't have enough samples of each kind!\n",
    "\n",
    "\n",
    "To illustrate the power of scikit-learn, lets pick another model. For this one, lets use a support vector machine. Recall from lecture that support vector machines (or SVM for short) attempt to draw an N-dimensional line between all the different classes (otherwise known as a hyperplane). Because our dataset looks pretty seperable, this should be easy for the model to do.\n",
    "\n",
    "The documentation for the SVM classifier is here: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we instantiate our model\n",
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we train the model on the training data and training labels\n",
    "model.fit(training_data, training_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we generate a numpy array of predictions for the testing data\n",
    "predicted_test_labels = model.predict(testing_data)\n",
    "predicted_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, we can do a simple comparison to the known test labels to get an accuracy\n",
    "sum(predicted_test_labels == testing_data_labels) / len(testing_data_labels) * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
